{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from matplotlib.pyplot import figure\n",
    "import time\n",
    "\n",
    "\n",
    "#define plotting style that I like\n",
    "style.use('seaborn-whitegrid')\n",
    "\n",
    "#define constants\n",
    "m_p=938.27231 #MeV\n",
    "m_n=939.56563 #MeV\n",
    "hbar_c=197.327053 #MeVfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fucntions to do necessary operations for creating Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define runtime calculator\n",
    "def timer(elapsed_time):\n",
    "    seconds = (time.time() - elapsed_time)\n",
    "    day = seconds // (24 * 3600)\n",
    "    seconds = seconds % (24 * 3600)\n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    seconds = seconds\n",
    "    print('')\n",
    "    print('Cell runtime:', \"%s days, %s hours, %s minutes, %s seconds\" % (day, hour, minutes, seconds) )\n",
    "    print('')\n",
    "    \n",
    "# fucntions for k for each rxn type\n",
    "def k_pp(energy):\n",
    "    value=np.sqrt(((m_p**2)*(energy)*(energy+2*m_n))/(((m_p+m_n)**2)+(2*energy*m_p))) \n",
    "    k_pp=value/hbar_c \n",
    "    return k_pp\n",
    "def k_np(energy):\n",
    "    value=np.sqrt((1/2)*m_p*energy) \n",
    "    k_np=value/hbar_c \n",
    "    return k_np\n",
    "\n",
    "# add k*experimental_value column\n",
    "def change_np(rxn_observable):\n",
    "    rxn_observable['K*experimental_value'] = rxn_observable.apply(lambda row: (row.experimental_value * k_np(row.energy)),axis=1)\n",
    "def change_pp(rxn_observable):\n",
    "    rxn_observable['K*experimental_value'] = rxn_observable.apply(lambda row: (row.experimental_value * k_pp(row.energy)),axis=1)\n",
    "\n",
    "def split_and_score(size, number_of_random_states):\n",
    "    test=np.zeros(number_of_random_states)\n",
    "    train=np.zeros(number_of_random_states)\n",
    "    for j in range(number_of_random_states):\n",
    "        #split\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=size)\n",
    "        #rf.train\n",
    "        rf = RandomForestRegressor(n_estimators=1000)\n",
    "        rf.fit(train_features, train_labels);\n",
    "        predictions = rf.predict(test_features)\n",
    "        rf_new = RandomForestRegressor(n_estimators = 100, criterion = 'mse', max_depth = None, min_samples_split = 2, min_samples_leaf = 1)\n",
    "        importances = list(rf.feature_importances_)\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "        #score\n",
    "        test[j]=rf.score(test_features,test_labels)\n",
    "        train[j]=rf.score(train_features,train_labels)\n",
    "    test_ave=np.mean(test)\n",
    "    train_ave=np.mean(train)\n",
    "    test_var=np.sqrt(np.var(test))\n",
    "    train_var=np.sqrt(np.var(train))\n",
    "    return test_ave, test_var, train_ave, train_var\n",
    "\n",
    "def train_and_plot(rxn_observable, reac_observ):\n",
    "    #start timer\n",
    "    rxn_observable_time = time.time()\n",
    "    # One-hot encode categorical features\n",
    "    rxn_observable_plot = pd.get_dummies(rxn_observable)\n",
    "    # Labels are the values we want to predict\n",
    "    labels = np.array(rxn_observable_plot['K*experimental_value'])\n",
    "    # Remove the labels from the features\n",
    "    # axis 1 refers to the columns\n",
    "    features = rxn_observable_plot.drop('K*experimental_value', axis = 1)\n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(features.columns)\n",
    "    # Convert to numpy array\n",
    "    features = np.array(features)\n",
    "\n",
    "    rxn_observable_test_mean=np.zeros(intervals) \n",
    "    rxn_observable_train_mean=np.zeros(intervals) \n",
    "    rxn_observable_test_variance=np.zeros(intervals) \n",
    "    rxn_observable_train_variance=np.zeros(intervals) \n",
    "    for i in range(intervals): \n",
    "        #define size\n",
    "        size=(i+1)/(intervals+1)\n",
    "        #call split function\n",
    "        test, test_v, train, train_v = split_and_score(size, number_of_random_states)\n",
    "        #add to arrays\n",
    "        rxn_observable_test_mean[i]=test\n",
    "        rxn_observable_train_mean[i]=train\n",
    "        rxn_observable_test_variance[i]=test_v\n",
    "        rxn_observable_train_variance[i]=train_v\n",
    "    print(\"Average Testing Score:\", rxn_observable_test_mean)\n",
    "    print('Testing Variance', rxn_observable_test_variance)\n",
    "    print('')\n",
    "    print('Average Training Score', rxn_observable_train_mean)\n",
    "    print('Training Variance', rxn_observable_train_variance)\n",
    "    print('')\n",
    "\n",
    "    #plot \n",
    "\n",
    "    #figure\n",
    "    figure(figsize=(15, 5))\n",
    "    ax=plt.subplot(111)\n",
    "    plt.xlabel('Test Size (%)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(\"Reaction_Observable: {}\".format(reac_observ))\n",
    "    plt.xlim(left=0, right=100)\n",
    "    ax.scatter(sample_size, rxn_observable_test_mean, s=100, label='Average Testing Score')\n",
    "    ax.scatter(sample_size, rxn_observable_train_mean, s=100, label='Average Training Score')\n",
    "    ax.errorbar(sample_size, rxn_observable_test_mean, yerr=rxn_observable_test_variance, linestyle='none', capsize=10, elinewidth=2)\n",
    "    ax.errorbar(sample_size, rxn_observable_train_mean, yerr=rxn_observable_train_variance, linestyle='none', capsize=10, elinewidth=2)\n",
    "    ax.legend(loc='best')\n",
    "    plt.show\n",
    "\n",
    "    #print elapsed time\n",
    "    timer(rxn_observable_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVERYTHING THAT NEEDS CHANGED SHOULD BE IN HERE\n",
    "\n",
    "#define number of random states\n",
    "number_of_random_states=5\n",
    "\n",
    "#define intervals of test size\n",
    "intervals=9 # 1:50%, 3:25%, 4:20%, 9:10%, 19:5%, 24:4%, 49:2%, 99:1%\n",
    "\n",
    "#x-axis\n",
    "if intervals==1:\n",
    "    sample_size=[50] #50% intervals\n",
    "elif intervals==3:\n",
    "    sample_size=list(range(25,76,25)) #25% intervals\n",
    "elif intervals==4:\n",
    "    sample_size=list(range(20,81,20)) #20% intervals \n",
    "elif intervals==9:\n",
    "    sample_size=list(range(10,91,10)) #10% intervals\n",
    "elif intervals==19:\n",
    "    sample_size=list(range(5,96,5)) #5% intervals\n",
    "elif intervals==24:\n",
    "    sample_size=list(range(4,97,4)) #4% intervals\n",
    "elif intervals==49:\n",
    "    sample_size=list(range(2,99,2)) #2% intervals\n",
    "elif intervals==99:\n",
    "    sample_size=list(range(1,100,1)) #1% intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>K*experimental_value</th>\n",
       "      <th>observable_   SGT</th>\n",
       "      <th>observable_   SGTT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.243152</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>22.491891</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000400</td>\n",
       "      <td>44.863041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     energy  K*experimental_value  observable_   SGT   observable_   SGTT\n",
       "0  0.000001              2.243152                   1                   0\n",
       "1  0.000100             22.491891                   1                   0\n",
       "2  0.000400             44.863041                   1                   0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data file\n",
    "features = pd.read_csv('CSV_files/np.sgt_and_sgtt.csv')\n",
    "#add k*exp column\n",
    "change_np(features)\n",
    "#remove columns that are not needed\n",
    "features = features.drop(columns=['scattering_angle', 'statistical_error', 'systematic_error', 'normalization', 'experimental_value', 'reaction_type'])\n",
    "features.head(3)\n",
    "\n",
    "#One-hot encode\n",
    "features = pd.get_dummies(features)\n",
    "features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 3 and input n_features is 1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a338f0720a09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Use the forest's predict method on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    386\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 3 and input n_features is 1 "
     ]
    }
   ],
   "source": [
    "# Labels are the values we want to predict\n",
    "labels = np.array(features['K*experimental_value'])\n",
    "# Remove the labels from the features\n",
    "features = features.drop('K*experimental_value', axis = 1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators= 1000)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "#make predictions \n",
    "#SGT\n",
    "E_SGT = \n",
    "SGT_predictions = rf.predict(E_SGT)\n",
    "#SGTT\n",
    "E_SGTT = \n",
    "SGTT_predictions = rf.predict(E_SGTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
